{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import logging\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from tqdm import tqdm as tqdm_notebook \n",
    "from tqdm import trange\n",
    "from tools import *\n",
    "import convert_examples_to_features\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler,\\\n",
    "                              TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM, BertForSequenceClassification\n",
    "from pytorch_pretrained_bert.optimization import BertAdam, WarmupLinearSchedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"./data/\"\n",
    "OUTPUT_DIR = './outputs/'\n",
    "LOG_DIR = \"./logs/\"\n",
    "SAVES_DIR = \"./saves/\"\n",
    "CACHE_DIR = \"./cache/\"\n",
    "WEIGHTS_NAME = \"pytorch_model.bin\"\n",
    "CONFIG_NAME = \"config.json\"\n",
    "\n",
    "MAX_SEQ_LENGTH = 128\n",
    "TRAIN_BATCH_SIZE = 24\n",
    "EVAL_BATCH_SIZE = 8\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_TRAIN_EPOCHS = 1\n",
    "RANDOM_SEED = 42\n",
    "GRADIENT_ACCUMULATION_STEPS = 1\n",
    "WARMUP_PROPORTION = 0.1\n",
    "OUTPUT_MODE = 'classification'\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaner(x):\n",
    "    return x.replace(r'\\n', ' ', regex=True)\n",
    "\n",
    "def to_tsv(X, filename = \"train\"):\n",
    "    prepro_X = pd.DataFrame({\n",
    "        'id': range(X.shape[0]),\n",
    "        'label': (X.choose_one == \"Relevant\")*1,\n",
    "        \"alpha\": ['a']*X.shape[0],\n",
    "        'text': cleaner(X.text)\n",
    "    })\n",
    "    prepro_X.to_csv(DATA_DIR + filename +'.tsv', sep='\\t', index=False, header=False)\n",
    "    return prepro_X\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "         id  label alpha                                               text\n10219     0      1     a  The sunset looked like an erupting volcano ......\n1413      1      1     a  #7294 Nikon D50 6.1 MP Digital SLR Camera Body...\n8887      2      0     a  Mental/Twitter Note: Make sure my smoke alarm ...\n4505      3      0     a  ?????? EMERGENCY ?????? NEED PART 2 and 3!!! #...\n8765      4      0     a  WHELEN MODEL 295SS-100 SIREN AMPLIFIER POLICE ...\n...     ...    ...   ...                                                ...\n2807   2171      1     a  'The Big Ten has their annual football media d...\n8685   2172      0     a  Does that sewer look like it's sinking to you?...\n8900   2173      0     a  sorry-I built a fire by my desk already. RT@ir...\n1543   2174      0     a  @CranBoonitz So going to make any bomb threats...\n6248   2175      0     a  #hot  Funtenna: hijacking computers to send da...\n\n[2176 rows x 4 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>label</th>\n      <th>alpha</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>10219</th>\n      <td>0</td>\n      <td>1</td>\n      <td>a</td>\n      <td>The sunset looked like an erupting volcano ......</td>\n    </tr>\n    <tr>\n      <th>1413</th>\n      <td>1</td>\n      <td>1</td>\n      <td>a</td>\n      <td>#7294 Nikon D50 6.1 MP Digital SLR Camera Body...</td>\n    </tr>\n    <tr>\n      <th>8887</th>\n      <td>2</td>\n      <td>0</td>\n      <td>a</td>\n      <td>Mental/Twitter Note: Make sure my smoke alarm ...</td>\n    </tr>\n    <tr>\n      <th>4505</th>\n      <td>3</td>\n      <td>0</td>\n      <td>a</td>\n      <td>?????? EMERGENCY ?????? NEED PART 2 and 3!!! #...</td>\n    </tr>\n    <tr>\n      <th>8765</th>\n      <td>4</td>\n      <td>0</td>\n      <td>a</td>\n      <td>WHELEN MODEL 295SS-100 SIREN AMPLIFIER POLICE ...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2807</th>\n      <td>2171</td>\n      <td>1</td>\n      <td>a</td>\n      <td>'The Big Ten has their annual football media d...</td>\n    </tr>\n    <tr>\n      <th>8685</th>\n      <td>2172</td>\n      <td>0</td>\n      <td>a</td>\n      <td>Does that sewer look like it's sinking to you?...</td>\n    </tr>\n    <tr>\n      <th>8900</th>\n      <td>2173</td>\n      <td>0</td>\n      <td>a</td>\n      <td>sorry-I built a fire by my desk already. RT@ir...</td>\n    </tr>\n    <tr>\n      <th>1543</th>\n      <td>2174</td>\n      <td>0</td>\n      <td>a</td>\n      <td>@CranBoonitz So going to make any bomb threats...</td>\n    </tr>\n    <tr>\n      <th>6248</th>\n      <td>2175</td>\n      <td>0</td>\n      <td>a</td>\n      <td>#hot  Funtenna: hijacking computers to send da...</td>\n    </tr>\n  </tbody>\n</table>\n<p>2176 rows × 4 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "X = pd.read_excel(\"./Classeur1.xlsx\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "X, X.choose_one, test_size=0.2, random_state=42)\n",
    "to_tsv(X_train, \"train\")\n",
    "to_tsv(X_test, \"dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>alpha</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10219</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>The sunset looked like an erupting volcano ......</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1413</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>#7294 Nikon D50 6.1 MP Digital SLR Camera Body...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8887</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>Mental/Twitter Note: Make sure my smoke alarm ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4505</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>?????? EMERGENCY ?????? NEED PART 2 and 3!!! #...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8765</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>WHELEN MODEL 295SS-100 SIREN AMPLIFIER POLICE ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2807</th>\n",
       "      <td>2171</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>'The Big Ten has their annual football media d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8685</th>\n",
       "      <td>2172</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>Does that sewer look like it's sinking to you?...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8900</th>\n",
       "      <td>2173</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>sorry-I built a fire by my desk already. RT@ir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1543</th>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>@CranBoonitz So going to make any bomb threats...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6248</th>\n",
       "      <td>2175</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>#hot  Funtenna: hijacking computers to send da...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2176 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  label alpha                                               text\n",
       "10219     0      1     a  The sunset looked like an erupting volcano ......\n",
       "1413      1      1     a  #7294 Nikon D50 6.1 MP Digital SLR Camera Body...\n",
       "8887      2      0     a  Mental/Twitter Note: Make sure my smoke alarm ...\n",
       "4505      3      0     a  ?????? EMERGENCY ?????? NEED PART 2 and 3!!! #...\n",
       "8765      4      0     a  WHELEN MODEL 295SS-100 SIREN AMPLIFIER POLICE ...\n",
       "...     ...    ...   ...                                                ...\n",
       "2807   2171      1     a  'The Big Ten has their annual football media d...\n",
       "8685   2172      0     a  Does that sewer look like it's sinking to you?...\n",
       "8900   2173      0     a  sorry-I built a fire by my desk already. RT@ir...\n",
       "1543   2174      0     a  @CranBoonitz So going to make any bomb threats...\n",
       "6248   2175      0     a  #hot  Funtenna: hijacking computers to send da...\n",
       "\n",
       "[2176 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False)\n",
    "processor = BinaryClassificationProcessor()\n",
    "train_examples = processor.get_train_examples(DATA_DIR)\n",
    "train_examples_len = len(train_examples)\n",
    "label_list = processor.get_labels() # [0, 1] for binary classification\n",
    "num_labels = len(label_list)\n",
    "num_train_optimization_steps = int(\n",
    "    train_examples_len / TRAIN_BATCH_SIZE / GRADIENT_ACCUMULATION_STEPS) * NUM_TRAIN_EPOCHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'i dont even remember slsp happening i just remember being like wtf and then the lights turned off and everyone screamed for the encore'"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "train_examples[0].text_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "label_map = {label: i for i, label in enumerate(label_list)}\n",
    "train_examples_for_processing = [(example, label_map, MAX_SEQ_LENGTH, tokenizer, OUTPUT_MODE) for example in train_examples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Preparing to convert 8700 examples..\nSpawning 15 processes..\n100%|██████████| 8700/8700 [01:57<00:00, 74.24it/s]\n"
    }
   ],
   "source": [
    "process_count = cpu_count() - 1\n",
    "if __name__ ==  '__main__':\n",
    "    print(f'Preparing to convert {train_examples_len} examples..')\n",
    "    print(f'Spawning {process_count} processes..')\n",
    "    with Pool(process_count) as p:\n",
    "        train_features = list(tqdm_notebook(p.imap(convert_examples_to_features.convert_example_to_feature, train_examples_for_processing), total=train_examples_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATA_DIR + \"train_features.pkl\", \"wb\") as f:\n",
    "    pickle.dump(train_features, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATA_DIR + \"train_features.pkl\", \"rb\") as f:\n",
    "    train_features = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_MODEL = 'bert-base-cased'\n",
    "# model = BertForSequenceClassification.from_pretrained(CACHE_DIR + 'cased_base_bert_pytorch.tar.gz', cache_dir=CACHE_DIR, num_labels=num_labels)\n",
    "model = BertForSequenceClassification.from_pretrained(BERT_MODEL, cache_dir=CACHE_DIR, num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "\n",
    "optimizer = BertAdam(optimizer_grouped_parameters,\n",
    "                     lr=LEARNING_RATE,\n",
    "                     warmup=WARMUP_PROPORTION,\n",
    "                     t_total=num_train_optimization_steps)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = 0\n",
    "nb_tr_steps = 0\n",
    "tr_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"***** Running training *****\")\n",
    "logger.info(\"  Num examples = %d\", train_examples_len)\n",
    "logger.info(\"  Batch size = %d\", TRAIN_BATCH_SIZE)\n",
    "logger.info(\"  Num steps = %d\", num_train_optimization_steps)\n",
    "all_input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)\n",
    "all_input_mask = torch.tensor([f.input_mask for f in train_features], dtype=torch.long)\n",
    "all_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long)\n",
    "\n",
    "\n",
    "all_label_ids = torch.tensor([f.label_id for f in train_features], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=TRAIN_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]\n0.761897\n0.767114\n0.673255\n0.857645\n0.531135\n0.789837\n0.732031\n0.641195\n0.657216\n0.633697\n0.600990\n0.643154\n0.503343\n0.684762\n0.487321\n0.573634\n0.438752\n0.588914\n0.429721\n0.555490\n0.519320\n0.467452\n0.483013\n0.437010\n0.503619\n0.350348\n0.423336\n0.477311\n0.410641\n0.538209\n0.349074\n0.910564\n0.652847\n0.818182\n0.457333\n0.561753\n0.476739\n0.530877\n0.884277\n0.400132\n0.522512\n0.420677\n0.516149\n0.534801\n0.510714\n0.510517\n0.420445\n0.604194\n0.270513\n0.376185\n0.500917\n0.543613\n0.609647\n0.498230\n0.588977\n0.476556\n0.447502\n0.722523\n0.547739\n0.308393\n0.395038\n0.390344\n0.365341\n0.284489\n0.625509\n0.603654\n0.431675\n0.287984\n0.435959\n0.459571\n0.579084\n0.575802\n0.562798\n0.488484\n0.467944\n0.428593\n0.528826\n0.455367\n0.239988\n0.413966\n0.395496\n0.353190\n0.651343\n0.694934\n0.433135\n0.468799\n0.565185\n0.342763\n0.523616\n0.455034\n0.315365\n0.428565\n0.457824\n0.355268\n0.532931\n0.478992\n0.551152\n0.637531\n0.445324\n0.471705\n0.537642\n0.578321\n0.400740\n0.523341\n0.585868\n0.252216\n0.505597\n0.449094\n0.411485\n0.360550\n0.311801\n0.509073\n0.512252\n0.388964\n0.582176\n0.376661\n0.407154\n0.522474\n0.439503\n0.447414\n0.560891\n0.382579\n0.540478\n0.321845\n0.381028\n0.479854\n0.428526\n0.489351\n0.354652\n0.411617\n0.565941\n0.331413\n0.444410\n0.394900\n0.271655\n0.733879\n0.504018\n0.547417\n0.357042\n0.346337\n0.593880\n0.398041\n0.381804\n0.353694\n0.297539\n0.309273\n0.594896\n0.497903\n0.422353\n0.292161\n0.415107\n0.546700\n0.466699\n0.403390\n0.600779\n0.521754\n0.616840\n0.426888\n0.278586\n0.372780\n0.282264\n0.566927\n0.602138\n0.346497\n0.473999\n0.500598\n0.414245\n0.451095\n0.222058\n0.302049\n0.371013\n0.366382\n0.542106\n0.467690\n0.531123\n0.408861\n0.633364\n0.730043\n0.635103\n0.211974\n0.495941\n0.556793\n0.610212\n0.564899\n0.343095\n0.485182\n0.613437\n0.327883\n0.483911\n0.365814\n0.418157\n0.711524\n0.431479\n0.553107\n0.200676\n0.352333\n0.593740\n0.529889\n0.422106\n0.425310\n0.583627\n0.283282\n0.231265\n0.333301\n0.406691\n0.464071\n0.380535\n0.403081\n0.271424\n0.384076\n0.407966\n0.544586\n0.669071\n0.454203\n0.541268\n0.481803\n0.442619\n0.396319\n0.622844\n0.371440\n0.360281\n0.526420\n0.529410\n0.622442\n0.240302\n0.474187\n0.355486\n0.392503\n0.394718\n0.325438\n0.456628\n0.306675\n0.458601\n0.416554\n0.445786\n0.220809\n0.591043\n0.377263\n0.353544\n0.385747\n0.245270\n0.415740\n0.399134\n0.749935\n0.448094\n0.408094\n0.307425\n0.294860\n0.624017\n0.155478\n0.378255\n0.367045\n0.164737\n0.293014\n0.403009\n0.338728\n0.439974\n0.577515\n0.311457\n0.418055\n0.271519\n0.333530\n0.557285\n0.370715\n0.497126\n0.374035\n0.258507\n0.399523\n0.365225\n0.398944\n0.555817\n0.587996\n0.372505\n0.386203\n0.379030\n0.433761\n0.600293\n0.470870\n0.288497\n0.205043\n0.350565\n0.412443\n0.561305\n0.274634\n0.335423\n0.553662\n0.460822\n0.267230\n0.395697\n0.375285\n0.459206\n0.461727\n0.406538\n0.713492\n0.310063\n0.458936\n0.403342\n0.395018\n0.309107\n0.548872\n0.522672\n0.431251\n0.263498\n0.527229\n0.423298\n0.207285\n0.370113\n0.383771\n0.417883\n0.330154\n0.280139\n0.417254\n0.374171\n0.374691\n0.551115\n0.598227\n0.363964\n0.502467\n0.582083\n0.435547\n0.364493\n0.371797\n0.539230\n0.406636\n0.428542\n0.594799\n0.418301\n0.397000\n0.465447\n0.313721\n0.441726\n0.440813\n0.372762\n0.565805\n0.440902\n0.537107\n0.449576\n0.379289\n0.216812\n0.469211\n0.348791\n0.662281\n0.483909\n0.568775\n0.442446\n0.395901\n0.382340\n0.585372\n0.354720\n0.364299\n0.354663\n0.263608\n0.276796\n0.425165\n0.480998\n0.319496\n0.438934\n0.441242\n0.466025\n0.413482\n0.369970\n0.440730\n0.477394\nIteration: 100%|██████████| 363/363 [03:43<00:00,  1.62it/s]\nEpoch: 100%|██████████| 1/1 [03:43<00:00, 223.73s/it]\n"
    }
   ],
   "source": [
    "model.train()\n",
    "for _ in trange(int(NUM_TRAIN_EPOCHS), desc=\"Epoch\"):\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    for step, batch in enumerate(tqdm_notebook(train_dataloader, desc=\"Iteration\")):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        input_ids, input_mask, segment_ids, label_ids = batch\n",
    "\n",
    "        logits = model(input_ids, segment_ids, input_mask, labels=None)\n",
    "        loss_fct = CrossEntropyLoss()\n",
    "        loss = loss_fct(logits.view(-1, num_labels), label_ids.view(-1))\n",
    "        \n",
    "        if GRADIENT_ACCUMULATION_STEPS > 1:\n",
    "            loss = loss / GRADIENT_ACCUMULATION_STEPS\n",
    "\n",
    "        loss.backward()\n",
    "        print(\"\\r%f\" % loss, end='')\n",
    "        \n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "        if (step + 1) % GRADIENT_ACCUMULATION_STEPS == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            global_step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "        _unit_id  _golden _unit_state  _trusted_judgments _last_judgment_at  \\\n8491   778253309    False   finalized                   5     8/27/15 16:07   \n7178   778251995    False   finalized                   5     8/27/15 20:16   \n2423   778247239    False   finalized                   5      8/30/15 0:15   \n10612  778255430    False   finalized                   5     8/27/15 17:03   \n10791  778255609    False   finalized                   5     8/27/15 22:11   \n\n         choose_one  choose_one:confidence choose_one_gold   keyword  \\\n8491   Not Relevant                 1.0000             NaN  screamed   \n7178   Not Relevant                 1.0000             NaN  mudslide   \n2423   Not Relevant                 1.0000             NaN   collide   \n10612      Relevant                 0.7978             NaN   wounded   \n10791  Not Relevant                 1.0000             NaN   wrecked   \n\n                        location  \\\n8491                         NaN   \n7178                   Edinburgh   \n2423                 planeta H2o   \n10612                        NaN   \n10791  Sunny Southern California   \n\n                                                    text       tweetid  \\\n8491   i dont even remember slsp happening i just rem...  6.291070e+17   \n7178   @hazelannmac ooh now I feel guilty about wishi...  6.290180e+17   \n2423   Soultech - Collide (Club Mix) http://t.co/8xIx...  6.290920e+17   \n10612  Police Officer Wounded Suspect Dead After Exch...  6.291190e+17   \n10791  Cramer: Iger's 3 words that wrecked Disney's s...  6.290800e+17   \n\n             userid  \n8491   2.327739e+08  \n7178   2.750220e+07  \n2423   6.052387e+08  \n10612  2.305930e+09  \n10791  2.464266e+07  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>_unit_id</th>\n      <th>_golden</th>\n      <th>_unit_state</th>\n      <th>_trusted_judgments</th>\n      <th>_last_judgment_at</th>\n      <th>choose_one</th>\n      <th>choose_one:confidence</th>\n      <th>choose_one_gold</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>tweetid</th>\n      <th>userid</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>8491</th>\n      <td>778253309</td>\n      <td>False</td>\n      <td>finalized</td>\n      <td>5</td>\n      <td>8/27/15 16:07</td>\n      <td>Not Relevant</td>\n      <td>1.0000</td>\n      <td>NaN</td>\n      <td>screamed</td>\n      <td>NaN</td>\n      <td>i dont even remember slsp happening i just rem...</td>\n      <td>6.291070e+17</td>\n      <td>2.327739e+08</td>\n    </tr>\n    <tr>\n      <th>7178</th>\n      <td>778251995</td>\n      <td>False</td>\n      <td>finalized</td>\n      <td>5</td>\n      <td>8/27/15 20:16</td>\n      <td>Not Relevant</td>\n      <td>1.0000</td>\n      <td>NaN</td>\n      <td>mudslide</td>\n      <td>Edinburgh</td>\n      <td>@hazelannmac ooh now I feel guilty about wishi...</td>\n      <td>6.290180e+17</td>\n      <td>2.750220e+07</td>\n    </tr>\n    <tr>\n      <th>2423</th>\n      <td>778247239</td>\n      <td>False</td>\n      <td>finalized</td>\n      <td>5</td>\n      <td>8/30/15 0:15</td>\n      <td>Not Relevant</td>\n      <td>1.0000</td>\n      <td>NaN</td>\n      <td>collide</td>\n      <td>planeta H2o</td>\n      <td>Soultech - Collide (Club Mix) http://t.co/8xIx...</td>\n      <td>6.290920e+17</td>\n      <td>6.052387e+08</td>\n    </tr>\n    <tr>\n      <th>10612</th>\n      <td>778255430</td>\n      <td>False</td>\n      <td>finalized</td>\n      <td>5</td>\n      <td>8/27/15 17:03</td>\n      <td>Relevant</td>\n      <td>0.7978</td>\n      <td>NaN</td>\n      <td>wounded</td>\n      <td>NaN</td>\n      <td>Police Officer Wounded Suspect Dead After Exch...</td>\n      <td>6.291190e+17</td>\n      <td>2.305930e+09</td>\n    </tr>\n    <tr>\n      <th>10791</th>\n      <td>778255609</td>\n      <td>False</td>\n      <td>finalized</td>\n      <td>5</td>\n      <td>8/27/15 22:11</td>\n      <td>Not Relevant</td>\n      <td>1.0000</td>\n      <td>NaN</td>\n      <td>wrecked</td>\n      <td>Sunny Southern California</td>\n      <td>Cramer: Iger's 3 words that wrecked Disney's s...</td>\n      <td>6.290800e+17</td>\n      <td>2.464266e+07</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./outputs/vocab.txt'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n",
    "\n",
    "# If we save using the predefined names, we can load using `from_pretrained`\n",
    "output_model_file = os.path.join(OUTPUT_DIR, WEIGHTS_NAME)\n",
    "output_config_file = os.path.join(OUTPUT_DIR, CONFIG_NAME)\n",
    "\n",
    "torch.save(model_to_save.state_dict(), output_model_file)\n",
    "model_to_save.config.to_json_file(output_config_file)\n",
    "tokenizer.save_vocabulary(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eval_report(task_name, labels, preds):\n",
    "    mcc = matthews_corrcoef(labels, preds)\n",
    "    tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()\n",
    "    return {\n",
    "        \"task\": task_name,\n",
    "        \"mcc\": mcc,\n",
    "        \"tp\": tp,\n",
    "        \"tn\": tn,\n",
    "        \"fp\": fp,\n",
    "        \"fn\": fn\n",
    "    }\n",
    "\n",
    "def compute_metrics(task_name, labels, preds):\n",
    "    assert len(preds) == len(labels)\n",
    "    return get_eval_report(task_name, labels, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(OUTPUT_DIR + 'vocab.txt', do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = BinaryClassificationProcessor()\n",
    "eval_examples = processor.get_dev_examples(DATA_DIR)\n",
    "label_list = processor.get_labels() # [0, 1] for binary classification\n",
    "num_labels = len(label_list)\n",
    "eval_examples_len = len(eval_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {label: i for i, label in enumerate(label_list)}\n",
    "eval_examples_for_processing = [(example, label_map, MAX_SEQ_LENGTH, tokenizer, OUTPUT_MODE) for example in eval_examples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing to convert 2176 examples..\n",
      "Spawning 15 processes..\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87f97034e7884b31b17290176094f15a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2176), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "process_count = cpu_count() - 1\n",
    "if __name__ ==  '__main__':\n",
    "    print(f'Preparing to convert {eval_examples_len} examples..')\n",
    "    print(f'Spawning {process_count} processes..')\n",
    "    with Pool(process_count) as p:\n",
    "        eval_features = list(tqdm_notebook(p.imap(convert_examples_to_features.convert_example_to_feature, eval_examples_for_processing), total=eval_examples_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)\n",
    "all_input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long)\n",
    "all_segment_ids = torch.tensor([f.segment_ids for f in eval_features], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if OUTPUT_MODE == \"classification\":\n",
    "    all_label_ids = torch.tensor([f.label_id for f in eval_features], dtype=torch.long)\n",
    "elif OUTPUT_MODE == \"regression\":\n",
    "    all_label_ids = torch.tensor([f.label_id for f in eval_features], dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "\n",
    "# Run prediction for full data\n",
    "eval_sampler = SequentialSampler(eval_data)\n",
    "eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=EVAL_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model (weights)\n",
    "model = BertForSequenceClassification.from_pretrained(OUTPUT_DIR, cache_dir=CACHE_DIR, num_labels=len(label_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)\n",
    "0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import matthews_corrcoef, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26b84896081042e3a13b0847b986164d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=272, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "eval_loss = 0\n",
    "nb_eval_steps = 0\n",
    "preds = []\n",
    "\n",
    "for input_ids, input_mask, segment_ids, label_ids in tqdm_notebook(eval_dataloader, desc=\"Evaluating\"):\n",
    "    input_ids = input_ids.to(device)\n",
    "    input_mask = input_mask.to(device)\n",
    "    segment_ids = segment_ids.to(device)\n",
    "    label_ids = label_ids.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids, segment_ids, input_mask, labels=None)\n",
    "\n",
    "    loss_fct = CrossEntropyLoss()\n",
    "    tmp_eval_loss = loss_fct(logits.view(-1, num_labels), label_ids.view(-1))\n",
    "    eval_loss += tmp_eval_loss.mean().item()\n",
    "    nb_eval_steps += 1\n",
    "    if len(preds) == 0:\n",
    "        preds.append(logits.detach().cpu().numpy())\n",
    "    else:\n",
    "        preds[0] = np.append(\n",
    "            preds[0], logits.detach().cpu().numpy(), axis=0)\n",
    "\n",
    "eval_loss = eval_loss / nb_eval_steps\n",
    "preds = preds[0]\n",
    "if OUTPUT_MODE == \"classification\":\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "elif OUTPUT_MODE == \"regression\":\n",
    "    preds = np.squeeze(preds)\n",
    "result = compute_metrics(\"test\", all_label_ids.numpy(), preds)\n",
    "\n",
    "result['eval_loss'] = eval_loss\n",
    "\n",
    "output_eval_file = os.path.join(\"./logs/\", \"eval_results.txt\")\n",
    "\n",
    "with open(output_eval_file, \"w\") as writer:\n",
    "    logger.info(\"***** Eval results *****\")\n",
    "    for key in (result.keys()):\n",
    "        logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "        writer.write(\"%s = %s\\n\" % (key, str(result[key])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Do you like pasta?\" in X.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = compute_metrics(\"test\", all_label_ids.numpy(), preds)\n",
    "\n",
    "result['eval_loss'] = eval_loss\n",
    "\n",
    "output_eval_file = os.path.join(\"./logs/\", \"eval_results.txt\")\n",
    "with open(output_eval_file, \"w\") as writer:\n",
    "    logger.info(\"***** Eval results *****\")\n",
    "    for key in (result.keys()):\n",
    "        logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "        writer.write(\"%s = %s\\n\" % (key, str(result[key])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}